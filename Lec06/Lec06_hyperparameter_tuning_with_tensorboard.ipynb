{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Experiments Management with Tensorboard\n",
    "=====================\n",
    "\n",
    "실제 딥러닝 연구나 개발을 하게 되면 꼭 마주치게 되는 것이 바로 수십개의 실험 결과를 관리하고 하이퍼파라미터에 따른 성능변화를 모니터링하고, 해당 실험 결과를 다시 재현하는 것입니다.  \n",
    "\n",
    "본 실습에서는 Tensorboard와 Hparam 기능을 활용해서 Cifar10 데이터셋에 대한 다수의 실험 결과를 관리하고 성능을 모니터링하는지 알아봅니다. \n",
    "\n",
    "Cifar 10 튜토리얼 코드는 [이곳](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)을 참조하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset과 DataLoader 준비하기  \n",
    "\n",
    "Cifar 10 데이터셋을 준비합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainset, _ = torch.utils.data.random_split(trainset, [5000, 45000])\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [4000, 1000])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testset, _ = torch.utils.data.random_split(testset, [2000, 8000])\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Construction\n",
    "\n",
    "간단한 CNN 아키텍쳐를 구현해봅니다.  \n",
    "두개의 `Conv` 모듈과 `MaxPool` 모듈을 통과한 후 `FC` 레이어를 통해 10개의 클래스를 분류합니다. 이 때 `Dropout` 모듈의 활성화 확률을 외부 변수로 입력 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.dropout = nn.Dropout(p=args.dp_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train, Validation, Test\n",
    "\n",
    "Data, Model, Loss, Optimization을 모두 같이 사용하여 봅시다. Epoch 별로 train과 validation, test가 이루어질 수 있게 함수를 나누었습니다. 이 때 train_loss, val_loss, accuracy가 기록되도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, (images, y) in enumerate(dataloader):\n",
    "        images, y = images.to(args.device), y.to(args.device)\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred_y = model(images)\n",
    "        train_loss = criterion(pred_y, y)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_train_loss /= len(dataloader)\n",
    "    return model, epoch_train_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion):\n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, y) in enumerate(dataloader):\n",
    "            images, y = images.to(args.device), y.to(args.device)\n",
    "\n",
    "            model.eval()\n",
    "            pred_y = model(images)\n",
    "            val_loss = criterion(pred_y, y)\n",
    "            epoch_val_loss += val_loss.item()\n",
    "\n",
    "    epoch_val_loss /= len(dataloader)\n",
    "    return epoch_val_loss\n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(args.device), labels.to(args.device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def experiment(partition, args):\n",
    "    \n",
    "    seed = 123\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    \n",
    "    model = Net(args)    \n",
    "    model.to(args.device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "      \n",
    "    args.best_acc = 0\n",
    "    for epoch in range(args.epoch):\n",
    "        model, train_loss = train(model, partition['train'], optimizer, criterion)\n",
    "        val_loss = validate(model, partition['val'], criterion)\n",
    "        accuracy = test(model, partition['test'])\n",
    "        \n",
    "        if accuracy > args.best_acc:\n",
    "            args.best_acc = accuracy\n",
    "            args.best_epoch = epoch\n",
    "            \n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Metric/acc', accuracy, epoch)\n",
    "        \n",
    "    writer.add_hparams(\n",
    "        hparam_dict=vars(args),\n",
    "        metric_dict={'best_acc':args.best_acc, 'best_epoch':args.best_epoch}\n",
    "    )\n",
    "                \n",
    "    return model, args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp  1] got acc: 36.850, at epoch  7\n",
      "[Exp  2] got acc: 43.400, at epoch  9\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "\n",
    "# ==== Model Architecture Config ==== #\n",
    "args.dp_rate = 0.3\n",
    "\n",
    "\n",
    "# ==== Optimizer Config ==== #\n",
    "args.lr = 0.00005\n",
    "args.l2_coef = 0.0001\n",
    "args.optim = 'ADAM'\n",
    "\n",
    "\n",
    "# ==== Training Config ==== #\n",
    "args.epoch = 10\n",
    "args.batch_size = 256\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.exp_name = 'exp1_lr_stage'\n",
    "\n",
    "\n",
    "# ==== DataLoader Preparation ==== #\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n",
    "                                          shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=args.batch_size,\n",
    "                                          shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size*2,\n",
    "                                         shuffle=False)\n",
    "partition = {'train': trainloader, 'val': valloader, 'test': testloader}\n",
    "\n",
    "\n",
    "# ==== Experiment ==== #\n",
    "#list_n_layer = [1]\n",
    "list_lr = [0.001, 0.005]\n",
    "\n",
    "cnt_exp = 0\n",
    "for lr in list_lr:\n",
    "    args.lr = lr\n",
    "\n",
    "    model, result = experiment(partition, args)\n",
    "\n",
    "    cnt_exp += 1\n",
    "    print('[Exp {:2}] got acc: {:2.3f}, at epoch {:2}'.format(cnt_exp, result.best_acc, result.best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "build_central"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
